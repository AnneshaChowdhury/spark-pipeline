<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Step 3 / Kafka Sensor Stream Consumer - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/img/favicon.ico"/>
<script>window.settings = {"sparkDocsSearchGoogleCx":"004588677886978090460:_rj0wilqwdm","dbcForumURL":"http://forums.databricks.com/","dbfsS3Host":"https://databricks-prod-storage-oregon.s3.amazonaws.com","enableThirdPartyApplicationsUI":true,"enableClusterAcls":false,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"isAdmin":false,"enablePresentationTimerConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"clusters":true,"hideOffHeapCache":false,"applications":false,"useStaticGuide":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"sparkVersions":[{"key":"1.3.x","displayName":"Spark 1.3.0","packageLabel":"spark-1.3-jenkins-ip-10-2-0-138-Ud074a3d8d1-S47b89c350f-2015-12-16-22:52:39.164963","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.4.x","displayName":"Spark 1.4.1","packageLabel":"spark-1.4-jenkins-ip-10-2-0-138-Ud074a3d8d1-S1a58b7577c-2015-12-16-22:52:39.164963","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-Ud074a3d8d1-S041018ecd7-2015-12-16-22:52:39.164963","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x","displayName":"Spark 1.6 Branch Preview","packageLabel":"spark-1.6-jenkins-ip-10-2-0-138-Ud074a3d8d1-S4e1cc60312-2015-12-16-22:52:39.164963","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":false,"enableFeedback":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.9.1","local":false,"displayDefaultContainerMemoryGB":30,"deploymentMode":"production","useSpotForWorkers":false,"enableStaticNotebooks":true,"dbcGuideURL":"#workspace/databricks_guide/00 Welcome to Databricks","enableClusterAclsConfig":false,"orgId":0,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":true,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableMavenLibraries":true,"defaultSparkVersion":{"key":"1.5.x","displayName":"Spark 1.5.2","packageLabel":"spark-1.5-jenkins-ip-10-2-0-138-Ud074a3d8d1-S041018ecd7-2015-12-16-22:52:39.164963","upgradable":true,"deprecated":false,"customerVisible":true},"clusterPublisherRootId":103404,"enableLatestJobRunResultPermalink":true,"enableSparkConfUI":true,"enableJdbcImport":true,"logfiles":"logfiles/","enableClusterDeltaUpdates":true,"csrfToken":"3ebd5da2-e868-4673-90b5-fda01f7789ce","useFixedStaticNotebookVersionForDevelopment":false,"enableBasicReactDialogBoxes":true,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableWorkspaceAclService":true,"enableWorkspaceAcls":true,"gitHash":"d074a3d8d132c947aa09c668f3458b5b7577b138","userFullname":"Burak","enableImportFromUrl":true,"enableMiniClusters":false,"enableWebSocketDeltaUpdates":true,"enableDebugUI":false,"showHiddenSparkVersions":false,"allowNonAdminUsers":true,"userId":100079,"dbcSupportURL":"http://help.databricks.com","staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/","enableSparkPackages":true,"enableNotebookHistoryUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"databricksGuideStaticUrl":"","notebookLoadingBackground":"#fff","enableNewJobRunDetailsPage":true,"enableDashboardExport":true,"user":"burak@databricks.com","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"tablesPublisherRootId":103410,"accounts":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":150962,"name":"Step 3 / Kafka Sensor Stream Consumer","language":"scala","commands":[{"version":"CommandV1","origId":150964,"guid":"53c516b1-323b-4333-a87a-97fe309c2cf4","subtype":"command","commandType":"auto","position":2.0,"command":"%md # **Kafka - Anomaly Detection **\n\nIn this notebook, we will receive data from Kafka. The data we receive for the purposes of this demo is request latency data. If some of our\nservers are taking longer and longer to respond, we will know about it in real time.\n\nThe end stream `areAbnormal` will contain information about which servers have taken longer to respond for more than 5 cycles. This stream\ncan then be used to send alerts (for example through email by using Amazon SES).\n\nWe will also power a dashboard of the latency data with the `sensorStream` stream.\n\nThe Kafka stream will have data in the format `(sensorName: String, latency: String)`. Latency is actually a `Double` value, but encoded as `String` for simplicity.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.437513932477E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"838d7ba1-9f15-4291-9ea8-8bad5d244f75"},{"version":"CommandV1","origId":150965,"guid":"b604b4bf-d50b-40f8-bd3b-f7cfa7322683","subtype":"command","commandType":"auto","position":3.0,"command":"%md ## Imports\n\nImport all the necessary libraries. If you see any error here, you have to make sure that you have attached the necessary libraries to the attached cluster. \n\nIn this particular notebook, make sure you have attached Maven dependencies `spark-streaming-kafka-assembly` for same version of Spark as your cluster.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.434007749487E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"td","iPythonMetadata":null,"nuid":"c01d3392-b608-40e3-90c5-c4171182b3f4"},{"version":"CommandV1","origId":150966,"guid":"32ffe74d-5e89-4ab5-9438-beb3a9b5b16a","subtype":"command","commandType":"auto","position":3.5,"command":"import java.nio.ByteBuffer\nimport scala.util.Random\n\nimport kafka.message.MessageAndMetadata\nimport kafka.serializer.StringDecoder\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.storage._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.dstream._\nimport org.apache.spark.streaming.kafka._","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.451049650128E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"burak@databricks.com","iPythonMetadata":null,"nuid":"9628ed52-72bd-4272-9930-3ce4b7ebeaf8"},{"version":"CommandV1","origId":151109,"guid":"c8c57135-a185-485f-9a65-f6cd6ff34cda","subtype":"command","commandType":"auto","position":3.71875,"command":"%md ## Setup: Define your Kafka configurations\n\nSetup your Kafka configurations, i.e. topics to listen to and list of brokers in the format `broker_ip:port`","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.433802139249E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null,"nuid":"3c29babc-b4c0-4a2f-8334-60eb34266b4e"},{"version":"CommandV1","origId":150968,"guid":"96b52400-c4c9-4c48-8bbb-27f3c8705400","subtype":"command","commandType":"auto","position":3.9375,"command":"val kafkaTopics = \"YOUR_KAFKA_TOPICS\"    // comma separated list of topics\nval kafkaBrokers = \"YOUR_KAFKA_BROKERS\"   // comma separated list of broker:host\n\ndef createKafkaStream(ssc: StreamingContext): DStream[(Int, Double)] = {\n  val topicsSet = kafkaTopics.split(\",\").toSet\n  val kafkaParams = Map[String, String](\"metadata.broker.list\" -> kafkaBrokers)\n  KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet).map { case (key, value) =>\n    (key.toInt, value.toDouble)\n  }\n}","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.451049651532E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"burak@databricks.com","iPythonMetadata":null,"nuid":"9c3f2779-37db-4cc6-8d7b-f72f699db706"},{"version":"CommandV1","origId":150969,"guid":"067196bf-8e7d-468d-ab97-cbae65292816","subtype":"command","commandType":"auto","position":4.703125,"command":"// Verify that the Kinesis settings have been set\nrequire(!kafkaTopic.contains(\"YOUR\"), \"Kafka topic have not been set\")\nrequire(!kafkaBrokers.contains(\"YOUR\"), \"Kafka brokers have not been set\")","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.451049653382E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"burak@databricks.com","iPythonMetadata":null,"nuid":"b9f12971-8e7a-477a-9a93-111bd7924a72"},{"version":"CommandV1","origId":150967,"guid":"621caf57-ab8f-4a4d-9056-6267d3e77635","subtype":"command","commandType":"auto","position":7.0,"command":"%md ## Setup: Define the function that sets up the StreamingContext\n\nThis function has to create a new StreamingContext and set it up with all the input stream, transformation and output operations.\n\nBut it must not start the StreamingContext, it is just to capture all the setup code necessary for your streaming application in one place.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.434007893568E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"td","iPythonMetadata":null,"nuid":"fc7ce3bc-e8a1-4d09-bbe4-241d316a94f0"},{"version":"CommandV1","origId":150970,"guid":"935a0024-33f9-44d8-ba91-2101da040fb4","subtype":"command","commandType":"auto","position":7.25,"command":"val batchIntervalSeconds = 1\nval checkpointDir = \"dbfs:/home/burak/kafka\"\n\n// Function to create a new StreamingContext and set it up\ndef creatingFunc(): StreamingContext = {\n    \n  // Create a StreamingContext\n  val ssc = new StreamingContext(sc, Seconds(batchIntervalSeconds))\n\n  // Get the latency stream from the source\n  val sensorStream = createKafkaStream(ssc)\n  \n  /** \n   * Helper function to calculate the mean latency and standard deviation of the latency for a given sensor.\n   * Uses the count, sum, and the sum of squares of all values seen so far to calculate these values.\n   */\n  def calculateMeanAndStd(count: Long, sum: Double, squaresSum: Double): (Double, Double) = {\n    val mean = sum / count\n    val variance = squaresSum / (count - 1) - (mean * mean / (count - 1)) * count\n    (mean, math.sqrt(variance))\n  }\n  \n  /** \n   * Our key is the sensor name. The value is the latency of the web service. We keep track of the count of\n   * measurements we received, the sum of the latencies so far and the sum of the squares of the latencies.\n   *\n   * We use these to calculate the running mean and standard deviation of the latencies. Then we use the mean\n   * and standard deviation to decide whether the current received latency fits a normal distribution with\n   * a confidence interval of 95%. If it does, we return 0 for that sensor. If it doesn't we return a 1.\n   *\n   * Notice that our mean and variance will continue updating, which means that if we don't take action with\n   * the alerts, then the mean and variance will adapt to the anomalous behavior.\n   */\n  def mappingFunction(key: Int, value: Option[Double], state: State[(Long, Double, Double)]): (Int, Int) = {\n    if (state.exists()) {\n      // get the existing state for the sensor\n      val (count, sum, sumSquares) = state.get()\n      if (count > 10) {\n        value.map { v =>\n          val (mean, std) = calculateMeanAndStd(count, sum, sumSquares)\n          state.update((count + 1, sum + v, sumSquares + (v * v)))\n          val lb = mean - 2 * std\n          val ub = mean + 2 * std\n          // decide whether value is outside our 95% confidence range\n          val isAbnormal = if (lb >= v || ub <= v) 1 else 0\n          (key, isAbnormal)\n        }.getOrElse((key, 0))\n      } else {\n        // if we don't have enough measurements (> 10), we just update the state and return a 0.\n        value.foreach { v =>\n          state.update((count + 1, sum + v, sumSquares + (v * v)))\n        }\n        (key, 0)\n      }\n    } else {\n      // The first time we received a measurement for this sensor. Update the state\n      // and return a 0.\n      value.map(v => state.update((1L, v, v * v)))\n      (key, 0)\n    }\n  }\n  \n  // Define the state spec using our mapping function\n  val spec = StateSpec.function(mappingFunction _).numPartitions(2)\n  \n  // This stream is not the state. Rather it is whether we deemed the latency value anomalous or not.\n  // The state can be accessed through `sensorAbnormalities.stateSnapshots()`, but we don't need it for this example.\n  val sensorAbnormalities = sensorStream.mapWithState[(Long, Double, Double), (Int, Int)](spec)\n  \n  // Decide whether we received more than 5 abnormal values in a sliding window of 10 seconds.\n  val areAbnormal = sensorAbnormalities.reduceByKeyAndWindow((a: Int, b: Int) => a + b, Seconds(10)).filter(_._2 > 5)\n  \n  // Print the abnormal sensors. Here we can trigger alerts, send emails, etc... using foreachRDD, and rdd.foreachPartition{...}.\n  areAbnormal.print()\n  \n  // Register the sensor data as a table for a window of 30 seconds, so that we can power a dashboard\n  sensorStream.transform { (rdd, time) =>\n    // Normally, the timestamp of the latency will be with the data itself, but for the purposes of the demo, we add them here\n    rdd.map(v => (v._1, v._2, time.milliseconds / 1000))\n  }.window(Seconds(30)).foreachRDD { (rdd, time) =>\n    // Turn the timestamp to a human readable time representation.\n    rdd.toDF(\"sensor\", \"latency\", \"timestamp\").withColumn(\"time\", from_unixtime($\"timestamp\", \"HH:mm:ss\")).registerTempTable(\"sensor_data\")\n  }\n  \n  // To make sure data is not deleted by the time we query it interactively\n  ssc.remember(Minutes(1))\n  \n  ssc.checkpoint(checkpointDir)\n  \n  println(\"Creating function called to create new StreamingContext\")\n  ssc\n}","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.451049662698E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"burak@databricks.com","iPythonMetadata":null,"nuid":"32440238-f1f7-4c56-a158-904906451a51"},{"version":"CommandV1","origId":150971,"guid":"f5cc6941-701d-4476-ab34-1bceb751df5e","subtype":"command","commandType":"auto","position":9.0,"command":"%md ## Start/Restart: Stop existing StreamingContext if any and start/restart the new one","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.437514804474E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"31f07116-1b26-426b-8bab-a80400468242"},{"version":"CommandV1","origId":150972,"guid":"1e6d26e6-d01a-40b9-a6de-704d850544aa","subtype":"command","commandType":"auto","position":10.859375,"command":"// Stop any existing StreamingContext \nval stopActiveContext = true\nif (stopActiveContext) {\t\n  StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }\n} \n\n// Get or create a streaming context.\nval ssc = StreamingContext.getActiveOrCreate(creatingFunc)\n\n// This starts the streaming context in the background. \nssc.start()\n\n// This is to ensure that we wait for some time before the background streaming job starts. This will put this cell on hold for 5 times the batchIntervalSeconds.\nssc.awaitTerminationOrTimeout(batchIntervalSeconds * 5 * 1000)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.451049665099E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"burak@databricks.com","iPythonMetadata":null,"nuid":"1c5056e6-a56d-410d-830b-5fe9891c7afa"},{"version":"CommandV1","origId":150973,"guid":"8f1d3078-9617-4d5f-8d39-8269d91c52f6","subtype":"command","commandType":"auto","position":11.125,"command":"%md ## Interactive Querying\n\nNow let's try querying the table. You can run this command again and again, you will find the numbers changing while data is being sent.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.434039556901E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"vida","iPythonMetadata":null,"nuid":"542328ca-b1f3-45bc-a604-4c4c0e7511d9"},{"version":"CommandV1","origId":150974,"guid":"f5eb22cc-a784-42d2-95ac-2b5242dd9e95","subtype":"command","commandType":"auto","position":11.359375,"command":"%sql select * from sensor_data","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.451052898457E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"lineChart","width":"660","height":"auto","xColumns":["time"],"yColumns":["latency"],"pivotColumns":["sensor"],"pivotAggregation":"avg","customPlotOptions":{"barChart":[{"key":"grouped","value":true},{"key":"stacked","value":false},{"key":"100_stacked","value":false}],"lineChart":[]},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"burak@databricks.com","iPythonMetadata":null,"nuid":"d0cb082c-73de-481f-91ee-567c834b80f7"},{"version":"CommandV1","origId":150975,"guid":"fc50d385-3239-4ae2-9fbc-3397fe552f64","subtype":"command","commandType":"auto","position":13.03125,"command":"%md ### Finally, if you want stop the StreamingContext, you can uncomment and execute the following","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.437514920595E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":null,"diffDeletes":null,"globalVars":{},"latestUser":"prakash@databricks.com","iPythonMetadata":null,"nuid":"9937dd64-9186-459a-a1ad-31e140128832"},{"version":"CommandV1","origId":150979,"guid":"ea151dac-8cdb-49fa-bd5b-8b58ad8f8dab","subtype":"command","commandType":"auto","position":14.03125,"command":"// StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"startTime":0.0,"submitTime":1.451049641871E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"burak@databricks.com","iPythonMetadata":null,"nuid":"b6c7c750-0ca1-45e1-88c7-1e22700790bf"}],"guid":"4e7bf489-661d-44b8-9268-c71b2cef6db3","globalVars":{},"iPythonMetadata":null};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"232ee453-4a45-41cd-8c55-82573f4824c6","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{},"plotOptions":null},"errorSummary":null,"error":null,"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201512151958120000-d074a3d8d132c947aa09c668f3458b5b7577b138/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>